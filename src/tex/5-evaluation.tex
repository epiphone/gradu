\chapter{Evaluation} \label{cha:evaluation}

This chapter evaluates the outcome of the migration process.
% The outcome of the migration process is evaluated in the fifth chapter. The potential benefits and drawbacks of the serverless platform outlined in chapter 2 are used to reflect on the final artifact. The chapter includes approximations on measurable attributes such as hosting costs and performance as well as discussion on the more subjective attributes like maintainability and testability. The overall ease of development -- or developer experience -- is also addressed since it is one of the commonly reported pain points of serverless computing

% Finally in chapter \ref{cha:evaluation} the migration outcome is evaluated against the pre-migration application in terms of quality and ease of development.
Evaluation the outcome of migration process. Estimate the effects on performance and hosting costs. Weigh in on maintainability, testability, developer experience etc.

\section{Developer perspective}

From a development point of view the process of rewriting server application code into FaaS function handlers was found relatively simple. The same runtime environment (NodeJS v10) was used in both versions of the application and the deployment artifacts for both were built from the same codebase. The server application's code benefited from being modularized so that e.g. the server-specific request handling implementation was separated from image processing logic. These factors together enabled a high degree of code reuse when implementing FaaS functions. The source code of the image labeling function is presented in listing \ref{lst:labelerhandler} to exemplify how a comparatively small amount of boilerplate code is required to instantiate a new function: in this function the actual labeling logic is imported from a \textit{label} module which is shared with the server application.

\begin{lstlisting}[language=JavaScript,caption=Image labeler function handler,captionpos=b,label=lst:labelerhandler,showstringspaces=false,belowskip=2em,frame=tb,aboveskip=2em]
  import { Storage } from '@google-cloud/storage'
  import { getBucketFileLabels } from '../label'

  const gcs = new Storage()

  export async function labeler(data) {
    const labels = await getBucketFileLabels(data)

    return gcs
      .bucket(data.bucket)
      .file(data.name)
      .setMetadata({ metadata: { Labels: labels } })
  }
\end{lstlisting}

Developing and testing the functions locally was found more challenging, depending on function type. In case of the authorizer function local development was simple since the function exposes a synchronous HTTP API, behaving the same as a conventional server application. In that case a local development server could be used to mimic a deployed function without any mocking or test harnesses required. Developing the other functions locally was more difficult due to their event-driven invocation style which meant that the triggering cloud events would have to be replicated locally. Without any tooling in place to mock cloud events, it was found easier to deploy these functions to the cloud and develop against the actual deployed functions. While the deployment process is relatively fast, this did incur a degree of overhead in the development cycle: in larger projects it can be worth the investment to set up tooling for mocking parts of the cloud environment in order to avoid this overhead in local development. In short, local development of conventional serverful web applications can be more straightforward than corresponding serverless ones, but the problem is not one of invoking FaaS functions but of replicating cloud service dependencies.

Image Manager's function deployment process consists of four steps: building a deployment artifact from the shared codebase, compressing it into a zip file, uploading the zip to Cloud Storage and finally updating the function to use the new deployment artifact. All together the process takes about one minute. This is significantly faster than pre-migration deployment which consisted of building a Docker image, uploading the image to an image repository and restarting the VM with the new image. The serverless Image Manager also has the advantage in easily deploying multiple co-existing versions of a single function, which is useful for testing or rolling out new features.

Compared to the pre-migration application, the serverless Image Manager has a clear advantage in isolation and modularization. Having split the monolithic server application into four distinct functions means that individual deployment units are smaller and have fewer dependencies and are thus more comprehensible and maintainable. The functions are decoupled from each other which makes extending the application painless: adding a new image processing task for example amounts to creating a new function and attaching it to the Cloud Storage image upload event -- no changes in other functions are required. This level of isolation also grants the ability to deploy features independently.

From a service monitoring point of view, the two versions of Image Manager act similarly. Application logs are persisted in Google Cloud's logging service which can be used for searching and setting up monitoring alarms. Both versions offer insight into similar performance metrics through the cloud provider dashboard, with the serverless version benefiting from more granular per-function metrics. The serverful application has the advantage in that the image upload sequence is contained inside a single HTTP request, which makes tracing application behaviour easier. As the serverless application is by nature more distributed and event-driven the end-to-end flow of a particular task can be more difficult to follow. Distributed logging, i.e. adding a unifying correlation identifier to logs that originate from different services but belong to the same logical task, can be of use here and is provided as a service in some platforms, e.g. AWS X-Ray \parencite{awslambda0218}.

\section{Performance perspective}

The two Image Manager implementations' performance characteristics were evaluated through a simple stress test. The goal of the test was to understand how the applications behave under heavy load and to identify the upper limits of both application's performance capacity. The test was carried out by sending a gradually increasing number of image processing requests to the target application in a simulation of a steadily increasing influx of user activity, using the Python-based load testing tool Locust. The authentication step was omitted from the test in order to focus on the resource-intensive parts of the application i.e. image processing. For comparability's sake both the serverful application's VM and each serverless function was allocated the same resources of a single CPU and 1024MB of memory. In order to minimize network delays and traffic costs the tests were ran on a separate VM deployed on the same Google Cloud region as both applications.

The serverful application's stress test results are plotted in diagram \ref{fig:serverfulStressTest}. The diagram presents the number of all requests, number of error responses and the average response time during the 11-minute test period. Analyzing the results shows how response times initially stay below four seconds until at around 300 cumulative requests, after which they start climbing up to 14 seconds or more. Moreover the initial curve on the number of total requests evens out into linear growth which indicates that the server is unable to cope with the increasing request rate, maxing out at around 3 requests per second. Eventually after 600 cumulative requests we observe a sharp spike in error responses due to server resource exhaustion. At this point the server application becomes nonresponsive as a majority of requests ends either in a server error or connection timeout. Figure \ref{fig:serverfulTrace} presents an example trace image to illustrate how a single request's lifetime is divided by the different operations and how the operations proceed either sequentially or concurrently: in a typical case the LQIP rendering and image labeling operations take up most of the time while thumbnail rendering and storage operations resolve quicker.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.9\textwidth]{serverful-load-test.png}
  \caption{Serverful Image Manager stress test results}
  \label{fig:serverfulStressTest}
\end{figure}

% TODO better trace image!
\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{image-manager-serverful-trace.png}
  \caption{Serverful Image Manager request trace}
  \label{fig:serverfulTrace}
\end{figure}

Running the same stress test against the serverless application yields differing results. Figure \ref{fig:serverlessStressTest} plots platform metrics captured during the test run, including the total function invocation rate per second, number of active instances per function and mean execution duration per function. First of all we observe the function invocation rate growing in tandem with the stress test's gradually increasing request rate. This is an expected outcome of the Event Processor pattern: as more images are uploaded to Cloud Storage, more processing tasks get triggered. The second chart illustrates the FaaS platform adapting to the growing invocation rate: an increasing number of function instances are spun up to handle invocations in parallel. While the sum of all of active instances peaks at around 100 we observe variance in scaling behaviour between the three functions, as the CPU-intensive LQIP rendering function scales up faster than the other two. Due to this scaling behaviour function execution durations stay constant while invocation rate grows from 0 to 60 per second, as plotted in the third chart. Interestingly the chart indicates slightly higher execution durations in the beginning of the test, which could be an indication of either function cold starts or the FaaS platform's underlying scaling strategy. With no sign of any performance degradation the test was aborted after 10 minutes.

% TODO establish the term "serverful"

Overall the two Image Manager implementations perform quite differently performance-wise, as expected. The serverful application's response times stay in the range of a few seconds until service resources are exhausted and rise sharply after that along with the error rate. The serverless application on the other hand adapts to increasing load with constant execution durations, presumably until the platform's concurrency limit is reached. While server response times and function execution durations are not directly comparable (the latter lacks image upload and event propagation latencies) they do manage to convey the drastic difference in elasticity.

The serverless Image Manager's elasticity, while mostly beneficial, points towards the necessity of protecting any less elastic downstream services with something like the Throttler pattern in order not to overwhelm them. The results also demonstrate the efficiency of the Bulkhead pattern's granular deployment units as the LQIP rendering operation can scale independently without allocating redundant resources elsewhere to the system.

As for the problem of cold starts, an initial delay in image processing was deemed acceptable and didn't thus necessitate implementing the Function Warmer pattern. The pattern can be useful in case of synchronously invoked functions such as the authorizer function where any delay leaks to the consumer and potentially blocks the user interface. Drawing from that, another workaround over the cold start problem is to design ``optimistic'' clients that immediately update the user interface after function invocation instead of waiting for a response.

% got more or less cloud-ready? \parencite{pozdniakova17cloudready}

% TODO use mean or average?

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.9\textwidth}
      \includegraphics[width=\textwidth]{cloud-functions-invocations-per-second.png}
      \caption{Invocations per second, sum of all functions}
      \label{fig:tiger}
  \end{subfigure}

  \begin{subfigure}[b]{0.9\textwidth}
      \includegraphics[width=\textwidth]{cloud-function-active-instances.png}
      \caption{Active instances per function}
      \label{fig:gull}
  \end{subfigure}

  \begin{subfigure}[b]{0.9\textwidth}
      \includegraphics[width=\textwidth]{cloud-function-execution-times-mean.png}
      \caption{Mean execution times per function}
      \label{fig:mouse}
  \end{subfigure}
  \caption{Serverless Image Manager stress test results}\label{fig:serverlessStressTest}
\end{figure}

\section{Economic perspective}

